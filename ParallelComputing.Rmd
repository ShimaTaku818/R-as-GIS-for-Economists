# Loop and Parallel Computing


```{r library_02,echo=FALSE,warning=FALSE}
suppressMessages(library(data.table))
suppressMessages(library(nycflights13))
suppressMessages(library(rgdal))
suppressMessages(library(rgeos))
suppressMessages(library(maptools))
suppressMessages(library(tictoc))
suppressMessages(library(raster))
suppressMessages(library(broom))
suppressMessages(library(tidyverse))
suppressMessages(library(stargazer))
suppressMessages(library(microbenchmark))
```

```{r chunk_set_02,echo=FALSE}
library(knitr)
opts_chunk$set(
  echo= TRUE,
  comment = NA,
  cache = TRUE, 
  message = FALSE,
  warning = FALSE,
  tidy=FALSE,
  #--- figure related ---#
  fig.align='center',
  fig.width=5,
  fig.height=4
  # dev='pdf'
  )
```


## Introduction

In this lecture, we will learn how to program repetitive operations effectively and fast. We sometimes need to run the same process over and over again often with slight changes in parameters. In such a case, it is very inefficient (time-consuming) and  messy to write all of the steps one bye one. For example, suppose you are interested in knowing the square of 1 through 5 in with a step of 1 ($[1, 2, 3, 4, 5]$). The following code certainly works:

```{r tedious}
1^2 
2^2 
3^2 
4^2 
5^2 
``` 

However, imagine you have to do this for 1000 integers. Yes, you don't want to write each one of them one by one as that would occupy 1000 lines of your code. Moreover, it's just tedious to do so. Things will be even worse if you need to repeat much more complicated processes like Monte Carlo simulations. So, let's learn how to write a program to do repetitive jobs effectively and fast. 

Here are the specific learning objectives of this chapter.

1. Learn how to use for loop and `lapply()` to complete repetitive jobs 
2. Learn how not to loop things that can be easily vectorized
3. Learn how to parallelize repetitive jobs using the `future_lapply()` function from the `future.apply` package

## What is loop? 

Looping is to repeatedly evaluate the same (except parameters) process over and over again. Here, the **same** process is the action of squaring. This does not change among the processes you run. What changes is what you square. There is a very efficient way of coding such processes. 

### For loop

Here is how **for loop** works in general:

```{r loop_explain, eval = FALSE}
for (x in a_list_of_values){
  you do what you want to do on x
}
```

As an example, let's use this looping syntax to do the same as above:

```{r loop}
for (x in 1:5){
  print(x^2)
}
```

Here, a list of values is $1:5$ ($[1, 2, 3, 4, 5]$). For each value of the list, you square it ($x^2$) and then print it ($print()$). If you want to get the square of $1:1000$, the only thing you need to change is the list of values as in:

```{r loop_more, eval=FALSE}
#--- evaluation not reported as it's too long ---#
for (x in 1:1000){
  print(x^2)
}
```

So, the length of the code does not depend on how many repeats you do, which is an obvious improvement over manual typing of every single process one by one. Note that you do not have to use $x$ to refer to an object you are going to use. It could be any combination of letters as long as you use it when you code what you want to do inside the loop. So, this would work just fine,

```{r silly_ex}
for (bluh_blugh_bluh in 1:5){
  print(bluh_blugh_bluh^2)
}
```

### While loop

Another type of loop is **while loop**. This is useful when you do not have priori a specific list of values to loop over and  want certain conditions to be satisfied before the repeated process is terminated. We do not talk about this loop much here. I will just provide one example that illustrates how this type of loop works. 

```{r while}
x <- 0
while (x < 6){
  print(x^2)
  x <- x + 1
}
```

### Looping using the `lapply()` function

You can do looping using the `lapply()` function^[`lpply()` in only one of the family of `apply()` functions. We do not talk about other types of `apply()` functions here (e.g., `apply()`, `spply()`, `mapply()`,, `tapply()`). Personally, I found myself only rarely using them. But, if you are interested in learning those, take a look at [here](https://www.datacamp.com/community/tutorials/r-tutorial-apply-family#gs.b=aW_Io) or [here](https://www.r-bloggers.com/using-apply-sapply-lapply-in-r/).]. In general, here is how it works:

$$
lapply(A,B)
$$

where $A$ is the list of values you go through one by one in the order the values are stored, and $B$ is the function you would like to apply to each of the values in $A$. For example, the following code does exactly the same thing as the above for loop example.

```{r lapply}
lapply(1:5, function(x){x^2})
```

Here, $A$ is $[1, 2, 3, 4, 5]$. In $B$ you have a function that takes $x$ and square it. So, the above code applies the function to each of $[1, 2, 3, 4, 5]$ one by one. In many circumstances, you can write the same looping actions in a much more concise manner using the `lapply` function than explicitly writing out the loop process as in the above for loop examples. You might have noticed that the output is a list. Yes, `lapply()` returns the outcomes in a list. That is where **l** in `lapply()` comes from.  

When the operation you would like to repeat becomes complicated (almost always the case), it is advisable that you create a function of that process first. 

```{r def_fcn}
#--- define the function first ---#
square_it <- function(x){
  return(x^2)
}

#--- lapply using the pre-defined function ---#
lapply(1:5, square_it)
```
  

### Looping over multiple variables using lapply()  

`lapply()` allows you to loop over only one variable. However, it is often the case that you want to loop over multiple variables^[the `map()` function from the `purrr` package allows you to loop over two variable.]. However, it is easy to achieve this. The trick is to create a `data.frame` of the variables where the complete list of the combinations of the variables are stored, and then loop over row of the `data.frame`. As an example, suppose we are interested in understanding the sensitivity of corn revenue to corn price and applied nitrogen amount. We consider the range of $3.0/bu to $5.0/bu for corn price and 0 lb/acre to 300/acre for applied nitrogen applied. 

```{r define_vectors}
#--- corn price vector ---#
corn_price_vec <- seq(3, 5, by = 1)

#--- nitrogen vector ---#
nitrogen_vec <- seq(0, 300, by = 100)
```

After creating vectors of the parameters, you combine them to create a complete combination of the parameters using the `expand.grid()` function, and then convert it to a `data.frame` object^[Converting to a `data.frame` is not strictly necessary.].

```{r param_mat}
#--- crate a data.frame that holds parameter sets to loop over ---#
parameters_data <- expand.grid(corn_price = corn_price_vec, nitrogen = nitrogen_vec) %>% 
  #--- convert the matrix to a data.frame ---#
  data.frame()

#--- take a look ---#
parameters_data
```

We now define a function that takes a row number, refer to `parameters_data` to extract the parameters stored at the row number, and then calculate corn yield and revenue based on the extracted parameters. 

```{r define_rev_function}
  gen_rev_corn <- function(i) {

    #--- define corn price ---#
    corn_price <- parameters_data[i,'corn_price']

    #--- define nitrogen  ---#
    nitrogen <- parameters_data[i,'nitrogen']

    #--- calculate yield ---#
    yield <- 240 * (1 - exp(0.4 - 0.02 * nitrogen))

    #--- calculate revenue ---#
    revenue <- corn_price * yield 

    #--- combine all the information you would like to have  ---#
    data_to_return <- data.frame(
      corn_price = corn_price,
      nitrogen = nitrogen,
      revenue = revenue
    )

    return(data_to_return)
  }
``` 

This function takes $i$ (act as a row number within the function), extract corn price and nitrogen from the $i$th row of `parameters_mat`, which are then used to calculate yield and revenue^[Yield is generated based on the Mitscherlich-Baule functional form. Yield increases at the decreasing rate as you apply more nitrogen, and yield eventually hits the plateau.]. Finally, it returns a `data.frame` of all the information you used (the parameters and the outcomes).

```{r revenue_data}
#--- loop over all the parameter combinations ---#
rev_data <- lapply(1:nrow(parameters_data), gen_rev_corn)

#--- take a look ---#
rev_data
```

Successful! Now, for us to use the outcome for other purposes like further analysis and visualization, we would need to have all the results combined into a single data.frame instead of a list of data.frames. To do this, use either `bind_rows()` from the `dplyr` package or `rbindlist()` from the `data.table` package.

```{r bind_rows}
#--- bind_rows ---#
bind_rows(rev_data)

#--- rbindlist ---#
rbindlist(rev_data)
```

### Do you really need to loop? 

Actually, we should not have used for loop or `lapply` in none of the examples above in practice^[By the way, note that `lapply` is no magic. It's basically a for loop and not rally any faster than for loop.] This is because they can be easily vectorized. Vectorized operations are those that take vectors as inputs and work on each element of the vectors in parallel^[This does not mean that the process is parallelized by using multiple cores.]. 

A typical example of a vectorized operation would be this:

```{r vec_1}
#--- define numeric vectors ---#
x <- 1:1000
y <- 1:1000

#--- element wise addition ---#
z_vec <- x + y 
```

A non-vectorized version of the same calculation is this:

```{r }
z_la <- lapply(1:1000, function(i) x[i] + y[i]) %>%  unlist

#--- check if identical with z_vec ---#
all.equal(z_la, z_vec)
```

Both produce the same results. However, R is written in a way that does much better doing vectorized operations. Let's time them using the `microbenchmark()` function from the `microbenchmark` package. Here, we do not `unlist()` after `lapply()` to just focus on the multiplication part.

```{r}
library(microbenchmark)

microbenchmark(
  #--- vectorized ---#
  "vectorized" = { x + y }, 
  #--- not vectorized ---#
  "not vectorized" = { lapply(1:1000, function(i) x[i] + y[i])},
  times = 100, 
  unit = "ms"
)
```

As you can see, the vectorized version is faster. The time difference comes from R having to conduct many more internal checks and hidden operations for the non-vectorized one^[See [this](http://www.noamross.net/archives/2014-04-16-vectorization-in-r-why/) or [this](https://stackoverflow.com/questions/7142767/why-are-loops-slow-in-r) to have a better understanding of why non-vectorized operations can be slower than vectorized operations.]. Yes, we are talking about a fraction of milliseconds here. But, as the objects to operated on get larger, the difference between vectorized and non-vectorized operations can become substantial^[see [here](http://www.win-vector.com/blog/2019/01/what-does-it-mean-to-write-vectorized-code-in-r/) for a good example of such a case. R is often regarded very slow compared to other popular software. But, many of such claims come from not vectorizing operations that can be vectorized. Indeed, many of the base and old R functions are written in C. More recent functions relies on C++ via the `Rcpp` package.]. 
    
The `lapply()` examples can be easily vectorized.        

Instead of this:

```{r lap_1, eval = FALSE}
lapply(1:1000 square_it)
```

You can just do this:

```{r vec_square, eval = FALSE}
square_it(1:1000)
```

You can also easily vectorize the revenue calculation demonstrated above. First, define the function differently so that revenue calculation can take corn price and nitrogen vectors and return a revenue vector.

```{r define_rev_simple}
  gen_rev_corn_short <- function(corn_price, nitrogen) {

    #--- calculate yield ---#
    yield <- 240 * (1 - exp(0.4 - 0.02 * nitrogen))

    #--- calculate revenue ---#
    revenue <- corn_price * yield 

    return(revenue)
  }
```  

Then use the function to calculate revenue and assign it to a new variable in the `parameters_data` data.

```{r no_need_to_loop}
 rev_data_2 <- mutate(
    parameters_data,
    revenue = gen_rev_corn_short(corn_price, nitrogen)
  ) 
```

Let's compare the two:

```{r }
microbenchmark(

  #--- vectorized ---#
  "vectorized" = { rev_data <- mutate(parameters_data, revenue = gen_rev_corn_short(corn_price, nitrogen)) },
  #--- not vectorized ---#
  "not vectorized" = { parameters_data$revenue <- lapply(1:nrow(parameters_data), gen_rev_corn) },
  times = 100, 
  unit = "ms"
)
```

Yes, the vectorized version is faster. So, the lesson here is that if you can vectorize, then vectorize instead of using `lapply()`. But, of course, things are just cannot be vectorized in many cases. 


## Parallelization of embarrassingly parallel processes

Parallelization of computation involves distribute the task at hand to multiple cores so that multiple processes are done in parallel. Here, we learn how to parallelize computation in R. Our focus is on the so called **embarrassingly** parallel processes. Embarrassingly parallel processes refers to a collection of processes where each process is completely independent of one another. That is, one process does not use the outputs of any of the other processes. The example of integer squaring is embarrassingly parallel. In order to calculate $1^2$, you do not need to use the result of $2^2$ or any other squares. Embarrassingly parallel processes are very easy to parallelize because you do not have to worry about which process to be completed first to make other processes happen. Fortunately, most of the processes you are interested in parallelizing fall under this category^[A good example of non-embarrassingly parallel process is dynamic optimization via backward induction. You need to know the optimal solution at $t = T$, before you find the optimal solution at $t = T-1$.].  

We will use the `future.apply` package for parallelization^[There are many other options including the `parallel`, `foreach` packages.]. Using the package, parallelizing is really a piece of cake as it is basically the same syntactically as `lapply()`. 

```{r load_fapply, message=FALSE, warning=FALSE}
#--- load packages ---#
library(future.apply) 
```

You can find out how many cores you have available for parallel computation on your computer using the `get_num_procs()` function from the `RhpcBLASctl` package.

```{r detect_cores}
library(RhpcBLASctl)

#--- number of all cores ---#
get_num_procs()
```

Before you implement parallelized `lapply()`, we need to declare what backend process we will be using the `plan()` function. Here, we use `plan(multiprocess)`^[If you are a Mac or Linux user, then the `multicore` process will be used, while the `multisession` process will be used if you are using a Windows machine. The `multicore` process is superior to the `multisession` process. See [this lecture note](https://raw.githack.com/uo-ec607/lectures/master/12-parallel/12-parallel.html) on parallel programming using R by Dr. Grant McDermott's at University of Oregon for the distinctions between the two and many other useful concepts for parallelization. At the time of writing, if you run R through RStudio, `multiprocess` option is always redirected to `multisession` option because of the instability in doing `multiprocess`. If you use Linux or Mac and want to take the full advantage of `future_apply`, you should run your R programs not through RStudio at least for now.]. In the `plan()` function, you can specify the number of workers. Here I will use the total number of cores less 1^[This way, you can have one more core available to do other tasks comfortably. However, if you don't mind having your computer completely devoted to the processing task at hand, then there is no reason not to use all the cores.].  

```{r plan}
plan(multiprocess, workers = get_num_procs() - 1)
```

`future_lapply()` works exactly like `lapply()`. 

```{r flapply}
sq_ls <- future_lapply(1:1000, function(x) x^2)
```

This is it. The only difference you see from the serialized processing using `lapply()` is that you changed the function name to `future_lapply()`. 

Okay, now we know how we parallelize computation. Let's check how much improvement in implementation time we got by parallelization. 

```{r do_mb}
microbenchmark(
  #--- parallelized ---#
  "parallelized" = { sq_ls <- future_lapply(1:1000, function(x) x^2) }, 
  #--- non-parallelized ---#
  "not parallelized" = { sq_ls <- lapply(1:1000, function(x) x^2) },
  times = 100, 
  unit = "ms"
)
```

Hmmmm, okay, so parallelization made the code slower... How could this be? This is because communicating jobs to each core takes some time as well. So, if each of the iterative processes is super fast (like this example where you just square a number), the time spent on communicating with the cores outweigh the time saving due to parallel computation. Parallelization is more beneficial when each of the repetitive processes takes long. 

One of the very good use cases of parallelization is MC simulation. The following MC simulation tests whether correlation between an independent variable and error term would cause bias (yes, we know the answer). The `MC_sim` function first generates a dataset (50,000 observations) according to the following data generating process:

$$
 y = 1 + x + v
$$
where $\mu \sim N(0,1)$, $x \sim N(0,1) + \mu$, and $v \sim N(0,1) + \mu$. The $\mu$ term cause correlation between $x$ (the covariate) and $v$ (the error term). It then estimates the coefficient on $x$ vis OLS, and return the estimate. We would like to repeat this process 1,000 times to understand the property of the OLS estimators under the data generating process. This Monte Carlo simulation is embarrassingly parallel because each process is independent of each other. 

```{r def_MC}
#--- repeat steps 1-3 B times ---#
MC_sim <- function(i){

  N <- 50000 # sample size

  #--- steps 1 and 2:  ---#
  mu <- rnorm(N) # the common term shared by both x and u
  x <- rnorm(N) + mu # independent variable
  v <- rnorm(N) + mu # error
  y <- 1 + x + v # dependent variable
  data <- data.table(y = y, x = x)

  #--- OLS ---# 
  reg <- lm(y~x, data = data) # OLS

  #--- return the coef ---#
  return(reg$coef['x'])
}
```

Let's run one iteration,

```{r one_run, eval = FALSE}
tic()
MC_sim(1)
toc()
```

```{r one_run_eval, echo = FALSE}
tic.clearlog()
tic()
MC_sim(1)
toc(log = TRUE, quiet = TRUE)
log_txt <- tic.log(format = FALSE)
time_elapsed <- log_txt[[1]]$toc - log_txt[[1]]$tic
time_elapsed
```

Okay, so it takes `r time_elapsed` second for one iteration. Now, let's run this 1000 times with or without parallelization.

**Not parallelized**

```{r benchmack_non_par, eval = FALSE}
#--- non-parallel ---#
tic()
sq_ls <- lapply(1:1000, MC_sim)
toc()
```

```{r benchmack_non_par_do, echo = FALSE}
#--- non-parallel ---#
tic.clearlog()
tic()
sq_ls <- lapply(1:1000, MC_sim)
toc(log = TRUE, quiet = TRUE)
log_txt <- tic.log(format = FALSE)
time_elapsed_ser <- log_txt[[1]]$toc - log_txt[[1]]$tic
time_elapsed_ser
```

**Parallelized**

```{r benchmack_par, eval = FALSE}
#--- parallel ---#
tic()
sq_ls <- future_lapply(1:1000, MC_sim)
toc()
```

```{r benchmack_par_do, echo = FALSE}
#--- parallel ---#
tic.clearlog()
tic()
sq_ls <- future_lapply(1:1000, MC_sim)
toc(log = TRUE, quiet = TRUE)
log_txt <- tic.log(format = FALSE)
time_elapsed_par <- log_txt[[1]]$toc - log_txt[[1]]$tic
time_elapsed_par
```

As you can see, parallelization makes it much quicker with a noticeable difference in elapsed time. We made the code `r round(time_elapsed_ser/time_elapsed_par, digit=2)` times faster. However, we did not make the process $7$ times faster even though we used 7 cores for the parallelized process. This is because of the overhead associated with distributing tasks to the cores. The relative advantage of parallelization would be even greater if each iteration takes more time. For example, if you are running a process that takes about 2 minutes for 1000 times, it would take approximately 33 hours and 20 minutes. But, it may take only 8 hours if you parallelize it on 7 cores, or maybe even 2 hours if you run it on 30 cores.

## Exercises

### Exercise 1: Vectorization and simple loop

We have two vectors:

```{r }
 x <- rnorm(1000)
 y <- rnorm(1000)
```


**Q 1.1:** Do vectorized addition of x and y (Method 1).

&nbsp;
&nbsp;

**Q 1.2:** Use the **lapply()** function to do element wise addition of x and y (Method 2).

&nbsp;
&nbsp;

**Q 1.3:** Parallelize the loop (Method 2) using the **future_lapply()** function (Method 3)

&nbsp;
&nbsp;

**Q 1.4** Using the **microbenchmark()** function, time all the three methods and identify the fastest method. Explain whey one is faster or slower than the others. 

&nbsp;
&nbsp;

## Exercise 2: Monte Carlo Simulation (small)

We know the sample average is an unbiased estimator of the expected value of a random variable. Let's confirm this numerically using Monte Carlo simulation. The random variable we will work on is distributed as $N(1, 1)$.

One iteration looks like this:

```{r , eval = FALSE}
#--- sample size ---#
N <- 1000
 
#--- sample from N(1,1) ---# 
x <- rnorm(N, mean = 1, sd = 1)

#--- get the sample mean ---#
mean(x)
```  

**Q 2.1** Conduct 1000 iterations of the same process using the **lapply** function and check if indeed sample mean is an unbiased estimator of the expected value. 

&nbsp;
&nbsp;


**Q 2.2** Parallelize the MC simulation using the **future_lapply()** function.

&nbsp;
&nbsp;


**Q 2.3** Using the **tic()** and **toc()** function to see which is faster.

&nbsp;
&nbsp;


## Exercise 3: Monte Carlo Simulation (larger)

For this exercise, change the sample size to $1000,000$ and redo the MC simulations using **lapply()** and **future_lapply()**, and finally confirm that the parallelized version is faster. Why is the parallelized version faster in this case while it was slower in the previous case?

&nbsp;
&nbsp;
&nbsp;
&nbsp;


```{r echo = FALSE, eval = FALSE}
get_mean <- function(i){

  #--- sample size ---#
  N <- 1000000
   
  #--- sample from N(1,1) ---# 
  x <- rnorm(N, mean = 1, sd = 1)

  #--- get the sample mean ---#
  mean(x)
}

tic()
res <- lapply(1:1000, get_mean)
toc()

library(future.apply)
plan(multiprocess, workers = 7)

tic()
res <- future_lapply(1:1000, get_mean)
toc()
```


## Exercise 4 (parallel computation)

**Note: This exercise requires knowledge on spatial data handling in R.** 

First, run the following code to get land use data form the Cropland Data Layer (CDL) dataset. 

```{r IA_CDL}
#--- load the cdlTools package ---#
library(cdlTools)

#--- download the CDL data for Iowa in 2015 ---#
IA_cdl_2015 <- getCDL("Iowa", 2015)$IA2015
```

Here is what the downloaded CDL data looks like: 

```{r plot_cdl}
#--- plot ---#
plot(IA_cdl_2015)
```

Next, run the following codes to create regular grids over Iowas state border.

```{r IA_grids}
library("sf")
library("maps")

#--- IA boundary ---#
IA_boundary <- st_as_sf(map("state", plot = FALSE, fill = TRUE)) %>% 
  #--- select Nebraska ---#
  filter(ID == "iowa") 

#--- create regular grids (40 cells by 40 columns) over Iowa ---#
IA_grids <- st_make_grid(IA_boundary, n = c(40, 40)) %>% 
  st_as_sf() %>% 
  #--- cut out the non-intersected parts of grids ---#
  st_intersection(., IA_boundary) %>% 
  #--- calculate the area of each grid ---#
  mutate(
    area = as.numeric(st_area(.)),
    area_ratio = area/max(area)
  ) %>% 
  #--- keep only if the area is large enough ---#
  filter(area_ratio > 0.8) %>% 
  #--- assign grid id for future merge ---#
  mutate(grid_id = 1:nrow(.)) %>% 
  #--- reproject to the CRS of the CDL data ---#
  st_transform(projection(IA_cdl_2015))

#--- plot the grids over the IA state border ---#
library(tmap)

tm_shape(IA_boundary) +
  tm_polygons(col = "green") +
tm_shape(IA_grids) +
  tm_polygons(alpha = 0)
```

The objective here is to get crop share by crop type for each of the grids in **IA_grids**. The final output should be an object that holds crop share information for all the grids in one dataset, which has grid identifier.  

**[Hint:]**  Here is how to get a crop share table for a grid. 

```{r }
temp_grid <- IA_grids[1, ]

crop_share <- crop(IA_cdl_2015, extent(temp_grid)) %>% 
  freq() 
```

**Q 4.1** Use the **lapply()** function to complete the task

&nbsp;
&nbsp;

**Q 4.2** Use the **future_lapply()** function to complete the task

&nbsp;
&nbsp;

---

```{r sessioninfo}
sessionInfo()
```