[
["index.html", "R as GIS for Economists Preface", " R as GIS for Economists Taro Mieno 2020-05-14 Preface This book is being developed as part of my effort to put together course materials for my data science course targeted at upper-level undergraduate and graduate students at the University of Nebraska Lincoln. This books aims particularly at spatial data processing for an econometric project, where spatial variables become part of an econometric analysis. Over the years, I have seen so many students and researchers who spend so much time just processing spatial data (often involving clicking the ArcGIS (or QGIS) user interface to death), which is a waste of time from the perspective of academic productivity. My hope is that this book will help researchers become more proficient in spatial data processing and enhance the overall productivity of the fields of economics for which spatial data are essential. About me I am an Assistant Professor at the Department of Agricultural Economics at University of Nebraska Lincoln, where I also teach Econometrics for Master’s students. My research interests lie in precision agriculture, water economics, and agricultural policy. My personal website is here. Comments and Suggestions? Any constructive comments and suggestions about how I can improve the book are all welcome. Please send me an email at tmieno2@unl.edu or create an issue on the github page of this book. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["why-r-as-gis-for-economists.html", "Why R as GIS for Economists?", " Why R as GIS for Economists? R has extensive capabilities as GIS software. In my opinion, \\(99\\%\\) of your spatial data processing needs as an economist will be satisfied by R. But, there are several popular options for GIS tasks other than R: Python ArcGIS QGIS Here I compare them briefly and discuss why R is a good option. R vs Python Both R and Python are actually heavily dependent on open source software GDAL and GEOS for their core GIS operations (GDAL for reading spatial data, and GEOS for geometrical operations like intersecting two spatial layers).1 So, when you run GIS tasks on R or Python you basically tell R or Python what you want to do and they talk to the software, let them do the job, and return the results back to us. This means that R and Python are not very different in its capability at GIS tasks as they are dependent on the common open source software for many GIS tasks. When GDAL and GEOS get better, R and Python get better (with a short lag). Both of them have good spatial visualization tools as well. Moreover, both R and Python can communicate with QGIS and ArcGIS (as long you as have them installed of course) and use their functionalities from within R and Python via the bridging packages: RQGIS and PyQGIS for QGIS, and R-ArcGIS and ArcPy.2 So, if you are more familiar with Python than R, go ahead and go for the Python option. From now on, my discussions assume that you are going for the R option, as otherwise, you would not be reading the rest of the book anyway. R vs ArcGIS or QGIS ArcGIS is commercial software and it is quite expensive (you are likely to be able to get a significant discount if you are a student at or work for a University). On the other hand, QGIS is open source and free. It has seen significant developments over the decade, and I would say it is just as competitive as ArcGIS. QGIS also uses open source geospatial software GDAL, GEOS, and others (SAGA, GRASS GIS). Both of them have a graphical interface that helps you implement various GIS tasks unlike R which requires programming. Now, since R can use ArcGIS and QGIS through the bridging packages, a more precise question we should be asking is whether you should program GIS tasks using R (possibly using the bridging packages) or manually implement GIS tasks using the graphical interface of ArcGIS or QGIS. The answer is programming GIS tasks using R. First, manual GIS operations are hard to repeat. It is often the case that in the course of a project you need to redo the same GIS task except that the underlying datasets have changed. If you have programmed the process with R, you just run the same code and that’s it. You get the desired results. If you did not program it, you need to go through many clicks on the graphical interface all over again, potentially trying to remember how you actually did it last time.3 Second and more important, manual operations are not scalable. It has become much more common that we need to process many large spatial datasets. Imagine you are doing the same operations on \\(1,000\\) files using a graphical interface, or even \\(50\\) files. Do you know who is good at doing the same tasks over and over again without complaining? A computer. Just let them do what they like to do. You have better things do. Finally, should you learn ArcGIS or QGIS in addition to (or before) R? I am doubtful. As economists, GIS tasks we need to do are not super convoluted most of the time. Suppose \\(\\Omega_R\\) and \\(\\Omega_{AQ}\\) represent the set of GIS tasks R and \\(ArcGIS/QGIS\\) can implement, respectively. Further, let \\(\\Omega_E\\) represent the set of skills economists need to implement. Then, \\(\\Omega_E \\in \\Omega_R\\) \\(99\\%\\) (or maybe \\(95\\%\\) to be safe) of the time and \\(\\Omega_E \\not\\subset \\Omega_{AQ}\\setminus\\Omega_R\\) \\(99\\%\\) of the time. Personally, I have never had to rely on either ArcGIS or QGIS for my research projects after I learn how to use R as GIS. One of the things ArcGIS and QGIS can do but R cannot do (\\(\\Omega_{AQ}\\setminus\\Omega_R\\)) is creating spatial objects by hand using a graphical user interface, like drawing polygons and lines. Another thing that R lags behind ArcGIS and QGIS is 3D data visualization. But, I must say neither of them is essential for economists at the moment. Finally, sometime it is easier and faster to make a map using ArcGIS and QGIS especially for a complicated map.4 Summary You have never used any GIS software? Learn R first. If you find out you really really cannot complete the tasks you would like to do on R, then turn to other options. You have used ArcGIS or QGIS and do not like them because they crash often? Why don’t you try R?5 You may realize you actually do not need them. You have used ArcGIS or QGIS before and are so comfortable with them, but need to program repetitive GIS tasks? Learn R and maybe take advantage of R-ArcGIS or RQGIS, which this book does not cover. You know for sure that you need to run only a simple GIS task once and never have to do any GIS tasks ever again? Stop reading and ask one of your friends to do the job. Pay him/her \\(\\$20\\) per hour, which is way below the opportunity cost of setting up either ArcGIS or QGI and learning to do that simple task on them. For example, see the very first sentence of this page↩ We do not learn them in this lecture note because I do not see the benefits of using them.↩ You could take a step by step note of what you did though.↩ Let me know if you know something that is essential for economists that only ArcGIS or QGIS can do. I will add that to the list her.↩ I am not saying R does not crash. R does crash. But, often times, the fault is yours, not the software’s.↩ "],
["how-is-this-book-different-from-other-online-books-and-resources.html", "How is this book different from other online books and resources?", " How is this book different from other online books and resources? We are seeing an explosion of online (and free) resources that teach how to use R for spatial data processing.6 Here is an incomplete list of such resources: Geocomputation with R Spatial Data Science Spatial Data Science with R Introduction to GIS using R Code for An Introduction to Spatial Analysis and Mapping in R Introduction to GIS in R Intro to GIS and Spatial Analysis Introduction to Spatial Data Programming with R Reproducible GIS analysis with R R for Earth-System Science Rspatial NEON Data Skills Simple Features for R Thanks to all these resources, it has become much easier to self-teach R for GIS work than six or seven years ago when I first started using R for GIS. Even though I have not read through all these resources carefully, I am pretty sure every topic found in this book can also be found somewhere in these resources (except the demonstrations). So, you may wonder why on earth you can benefit from reading this book. It all boils down to search costs. Researchers in different disciplines require different sets of spatial data skills. The available resources are either very general covering so many topics that economists are very unlikely to use. It is particularly hard for those who do not have much experience in GIS to identify whether particular skills are essential or not. So, they could spend so much time learning something that is not really useful. The value of this book lies in its deliberate incomprehensiveness. It only packages materials that satisfy the need of most economists, cutting out many topics that are likely to be of limited use for economists. For those who are looking for more comprehensive treatments of spatial data handling and processing in one book, I personally like Geocomputation with R a lot. Increasingly, the developer of R packages created a website dedicated to their R packages, where you can often find vignettes (tutorials), like Simple Features for R. This phenomenon is largely thanks to packages like bookdown (Xie 2016), blogdown (Xie, Hill, and Thomas 2017), and pkgdown (Wickham and Hesselberth 2020) that has lowered the cost of professional contents creation much much lower than before. Indeed, this book was built taking advantage of the bookdown package.↩ "],
["what-is-going-to-be-covered-in-this-book.html", "What is going to be covered in this book?", " What is going to be covered in this book? The book starts with the very basics of spatial data handling (e.g., importing and exporting spatial datasets) and moves on to more practical spatial data operations (e.g., spatial data join) that are useful for research projects. This books is still under development. Right now, only Chapter 1 is available. I will work on the rest of the book over the summer. The “coming soon” chapters are close to be done. I just need to add finishing touches to those chapters. The “wait a bit” chapters need some more work, adding contents, etc. Chapter 1: Demonstrations of R as GIS (available) groundwater pumping and groundwater level precision agriculture land use and weather corn planted acreage and railroads groundwater pumping and weather Chapter 2: The basics of vector data handling using sf package (coming soon) spatial data structure in sf import and export vector data (re)projection of spatial datasets single-layer geometrical operations (e.g., create buffers, find centroids) other miscellaneous basic operations Chapter 3: Spatial interactions of vector datasets (coming soon) spatially subsetting one layer based on another layer extracting values from one layer to another layer7 Chapter 4: The basics of raster data handling using raster and terra packages (coming soon) import and export raster data stacking raster data Chapter 5: Spatial interactions of vector and raster datasets (wait a bit) extracting values from a raster layer to a vector layer Chapter 6: Efficient spatial data processing (wait a bit) parallelization Chapter 7: Downloading publicly available spatial datasets (wait a bit) Sentinel 2 (sen2r) USDA NASS QuickStat (tidyUSDA) PRISM (prism) Daymet (daymetr) USGS (dataRetrieval) Chapter 8: Parallel computation (wait a bit) As you can see above, this book does not spend any time on the very basics of GIS concepts. Before you start reading the book, you should know the followings at least (it’s not much): What Geographic Coordinate System (GCS), Coordinate Reference System (CRS), and projection are (this is a good resource) Distinctions between vector and raster data (this is a simple summary of the difference) Finally, this book does not cover spatial statistics or spatial econometrics at all. This book is about spatial data processing. Spatial analysis is something you do after you have processed spatial data. over function in sp language↩ "],
["conventions-of-the-book-and-some-notes.html", "Conventions of the book and some notes", " Conventions of the book and some notes Here are some notes of the conventions of this book and notes for R beginners and those who are not used to reading rmarkdown-generated html documents. Texts in gray boxes They are one of the following: objects defined on R during demonstrations R functions R packages When it is a function, I always put parentheses at the end like this: st_read().8 Sometimes, I combine a package and function in one like this: sf::st_read(). This means it is a function called st_read() from the sf package. Colored Boxes Codes are in blue boxes, and outcomes are in red boxes. Codes: runif(5) Outcomes: ## [1] 0.6620285 0.5236162 0.7255064 0.3159363 0.6393840 Parentheses around codes Sometimes you will see codes enclosed by parenthesis like this: ( a &lt;- runif(5) ) ## [1] 0.04843399 0.51585740 0.61120891 0.01559738 0.03947177 The parentheses prints what’s inside of a newly created object (here a) without explicitly evaluating the object. So, basically I am signaling that we will be looking inside of the object that was just created. This one prints nothing. a &lt;- runif(5) Footnotes Footnotes appear at the bottom of the page. You can easily get to a footnote by clicking on the footnote number. You can also go back to the main narrative where the footnote number is by clicking on the curved arrow at the end of the footnote. So, don’t worry about having to scroll all the way up to where you were after reading footnotes. This is a function that draws values randomly from the uniform distribution.↩ "],
["session-information.html", "Session Information", " Session Information Here is the session information when compiling the book: sessionInfo() ## R version 4.0.0 (2020-04-24) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Mojave 10.14.1 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_4.0.0 magrittr_1.5 bookdown_0.18 htmltools_0.4.0 ## [5] tools_4.0.0 yaml_2.2.1 Rcpp_1.0.4.6 stringi_1.4.6 ## [9] rmarkdown_2.1 knitr_1.28 stringr_1.4.0 digest_0.6.25 ## [13] xfun_0.13 rlang_0.4.6 evaluate_0.14 "],
["demo.html", "Chapter 1 R as GIS: Demonstrations ", " Chapter 1 R as GIS: Demonstrations "],
["introduction.html", "Introduction", " Introduction The primary objective of this chapter is to showcase the power of R as GIS through demonstrations using mock-up econometric research projects9. Each project consists of a project overview (objective, datasets used, econometric model, and GIS tasks involved) and demonstration. This is really not a place you learn the nuts and bolts of how R does spatial operations. Indeed, we intentionally do not explain all the details of how the R codes work. We reiterate that the main purpose of the demonstrations is to get you a better idea of how R can be used to process spatial data to help your research projects involving spatial datasets. Finally, note that these mock-up projects use extremely simple econometric models that completely lacks careful thoughts you would need in real research projects. So, don’t waste your time judging the econometric models, and just focus on GIS tasks. If you are not familiar with html documents generated by rmarkdown, you might benefit from reading the conventions of the book in the Preface. Target Audience The target audience of this chapter is those who are not very familiar with R as GIS. Knowledge of R certainly helps. But, I tried to write in a way that R beginners can still understand the power of R as GIS10. Do not get bogged down by all the complex-looking R codes. Just focus on the narratives and figures to get a sense of what R can do. Direction for replication Running the codes in this chapter involves reading datasets from a disk. All the datasets that will be imported are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps:11 set a folder (any folder) as the working directory using setwd() create a folder called “Data” inside the folder designated as the working directory download the pertinent datasets from here and put them in the “Data” folder run Chap_1_Demonstration.R which is included in the datasets folder you have downloaded source(&quot;Data/Chap_1_Demonstration.R&quot;) Note that the data folder includes 183-day worth of PRISM precipitation data for Demonstration 3, which are quite large in size (slightly less than 1 GB). If you are not replicating Demonstration 3, you can either choose not to download them or discard them if you have downloaded them already. Note that this lecture does not deal with spatial econometrics at all. This lecture is about spatial data processing, not spatial econometrics. This is a great resource for spatial econometrics in R.↩ I welcome any suggestions to improve the reading experience of unexperienced R users.↩ I thought about using the here package, but I found it a bit confusing for unexperienced R users.↩ "],
["demonstration-1-the-impact-of-groundwater-pumping-on-depth-to-water-table.html", "1.1 Demonstration 1: The impact of groundwater pumping on depth to water table", " 1.1 Demonstration 1: The impact of groundwater pumping on depth to water table .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.1.1 Project Overview Objective: Understand the impact of groundwater pumping on groundwater level. Datasets Groundwater pumping by irrigation wells in Chase, Dundy, and Perkins Counties in the southwest corner of Nebraska Groundwater levels observed at USGS monitoring wells located in the three counties and retrieved from the National Water Information System (NWIS) maintained by USGS using the dataRetrieval package. Econometric Model In order to achieve the project objective, we will estimate the following model: \\[ y_{i,t} - y_{i,t-1} = \\alpha + \\beta gw_{i,t-1} + v \\] where \\(y_{i,t}\\) is the depth to groundwater table12 in March13 in year \\(t\\) at USGS monitoring well \\(i\\), and \\(gw_{i,t-1}\\) is the total amount of groundwater pumping that happened within the 2-mile radius of the monitoring well \\(i\\). GIS tasks read an ESRI shape file as an sf (spatial) object use sf::st_read() download depth to water table data using the dataRetrieval package developed by USGS use dataRetrieval::readNWISdata() and dataRetrieval::readNWISsite() create a buffer around USGS monitoring wells use sf::st_buffer() convert a regular data.frame (non-spatial) with geographic coordinates into an sf (spatial) objects use sf::st_as_sf() and sf::st_set_crs() reproject an sf object to another CRS use sf::st_transform() identify irrigation wells located inside the buffers and calculate total pumping use sf::st_join() packages Load (install first if you have not) the following packages if you intend to replicate the demonstration. library(sf) library(dplyr) library(lubridate) library(stargazer) There are other packages that will be loaded during the demonstration. 1.1.2 Project Demonstration The geographic focus of the project is the southwest corner of Nebraska consisting of Chase, Dundy, and Perkins County (see Figure 1.1 for their locations within Nebraska). Let’s read a shape file of the three counties represented as polygons. We will use it later to spatially filter groundwater level data downloaded from NWIS. three_counties &lt;- st_read(dsn = &quot;./Data&quot;, layer = &quot;urnrd&quot;) %&gt;% #--- project to WGS84/UTM 14N ---# st_transform(32614) Reading layer `urnrd&#39; from data source `/Users/tmieno2/Dropbox/TeachingUNL/RGIS_Econ/Data&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 3 features and 1 field geometry type: POLYGON dimension: XY bbox: xmin: -102.0518 ymin: 40.00257 xmax: -101.248 ymax: 41.00395 CRS: 4269 Figure 1.1: The location of Chase, Dundy, and Perkins County in Nebraska We have already collected groundwater pumping data, so let’s import it. #--- groundwater pumping data ---# ( urnrd_gw &lt;- readRDS(&quot;./Data/urnrd_gw_pumping.rds&quot;) ) well_id year vol_af lon lat 1: 1706 2007 182.566 245322.3 4542717 2: 2116 2007 46.328 245620.9 4541125 3: 2583 2007 38.380 245660.9 4542523 4: 2597 2007 70.133 244816.2 4541143 5: 3143 2007 135.870 243614.0 4541579 --- 18668: 2006 2012 148.713 284782.5 4432317 18669: 2538 2012 115.567 284462.6 4432331 18670: 2834 2012 15.766 283338.0 4431341 18671: 2834 2012 381.622 283740.4 4431329 18672: 4983 2012 NA 284636.0 4432725 well_id is the unique irrigation well identifier, and vol_af is the amount of groundwater pumped in acre-feet. This dataset is just a regular data.frame with coordinates. We need to convert this dataset into a object of class sf so that we can later identify irrigation wells located within a 2-mile radius of USGS monitoring wells (see Figure 1.2 for the spatial distribution of the irrigation wells). urnrd_gw_sf &lt;- urnrd_gw %&gt;% #--- convert to sf ---# st_as_sf(coords = c(&quot;lon&quot;, &quot;lat&quot;)) %&gt;% #--- set CRS WGS UTM 14 (you need to know the CRS of the coordinates to do this) ---# st_set_crs(32614) #--- now sf ---# urnrd_gw_sf Simple feature collection with 18672 features and 3 fields geometry type: POINT dimension: XY bbox: xmin: 239959 ymin: 4431329 xmax: 310414.4 ymax: 4543146 CRS: EPSG:32614 First 10 features: well_id year vol_af geometry 1 1706 2007 182.566 POINT (245322.3 4542717) 2 2116 2007 46.328 POINT (245620.9 4541125) 3 2583 2007 38.380 POINT (245660.9 4542523) 4 2597 2007 70.133 POINT (244816.2 4541143) 5 3143 2007 135.870 POINT (243614 4541579) 6 5017 2007 196.799 POINT (243539.9 4543146) 7 1706 2008 171.250 POINT (245322.3 4542717) 8 2116 2008 171.650 POINT (245620.9 4541125) 9 2583 2008 46.100 POINT (245660.9 4542523) 10 2597 2008 124.830 POINT (244816.2 4541143) Figure 1.2: Spatial distribution of irrigation wells Here are the rest of the steps we will take to obtain a regression-ready dataset for our analysis. download groundwater level data observed at USGS monitoring wells from National Water Information System (NWIS) using the dataRetrieval package identify the irrigation wells located within the 2-mile radius of the USGS wells and calculate the total groundwater pumping that occurred around each of the USGS wells by year merge the groundwater pumping data to the groundwater level data Let’s download groundwater level data from NWIS first. The following code downloads groundwater level data for Nebraska from Jan 1, 1990, through Jan 1, 2016. #--- load the dataRetrieval package ---# library(dataRetrieval) #--- download groundwater level data ---# NE_gwl &lt;- readNWISdata( stateCd=&quot;Nebraska&quot;, startDate = &quot;1990-01-01&quot;, endDate = &quot;2016-01-01&quot;, service = &quot;gwlevels&quot; ) %&gt;% dplyr::select(site_no, lev_dt, lev_va) %&gt;% rename(date = lev_dt, dwt = lev_va) #--- take a look ---# head(NE_gwl, 10) site_no date dwt 1 400008097545301 2000-11-08 17.40 2 400008097545301 2008-10-09 13.99 3 400008097545301 2009-04-09 11.32 4 400008097545301 2009-10-06 15.54 5 400008097545301 2010-04-12 11.15 6 400008100050501 1990-03-15 24.80 7 400008100050501 1990-10-04 27.20 8 400008100050501 1991-03-08 24.20 9 400008100050501 1991-10-07 26.90 10 400008100050501 1992-03-02 24.70 site_no is the unique monitoring well identifier, date is the date of groundwater level monitoring, and dwt is depth to water table. We calculate the average groundwater level in March by USGS monitoring well (right before the irrigation season starts):14 #--- Average depth to water table in March ---# NE_gwl_march &lt;- NE_gwl %&gt;% mutate( date = as.Date(date), month = month(date), year = year(date), ) %&gt;% #--- select observation in March ---# filter(year &gt;= 2007, month == 3) %&gt;% #--- gwl average in March ---# group_by(site_no, year) %&gt;% summarize(dwt = mean(dwt)) #--- take a look ---# head(NE_gwl_march, 10) # A tibble: 10 x 3 # Groups: site_no [2] site_no year dwt &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 400032101022901 2008 118. 2 400032101022901 2009 117. 3 400032101022901 2010 118. 4 400032101022901 2011 118. 5 400032101022901 2012 118. 6 400032101022901 2013 118. 7 400032101022901 2014 116. 8 400032101022901 2015 117. 9 400038099244601 2007 24.3 10 400038099244601 2008 21.7 Since NE_gwl is missing geographic coordinates for the monitoring wells, we will download them using the readNWISsite() function and select only the monitoring wells that are inside the three counties. #--- get the list of site ids ---# NE_site_ls &lt;- NE_gwl$site_no %&gt;% unique() #--- get the locations of the site ids ---# sites_info &lt;- readNWISsite(siteNumbers = NE_site_ls) %&gt;% dplyr::select(site_no, dec_lat_va, dec_long_va) %&gt;% #--- turn the data into an sf object ---# st_as_sf(coords = c(&quot;dec_long_va&quot;, &quot;dec_lat_va&quot;)) %&gt;% #--- NAD 83 ---# st_set_crs(4269) %&gt;% #--- project to WGS UTM 14 ---# st_transform(32614) %&gt;% #--- keep only those located inside the three counties ---# .[three_counties, ] We now identify irrigation wells that are located within the 2-mile radius of the monitoring wells15. We first create polygons of 2-mile radius circles around the monitoring wells (see Figure 1.3). buffers &lt;- st_buffer(sites_info, dist = 2*1609.34) # in meter Figure 1.3: 2-mile buffers around USGS monitoring wells We now identify which irrigation wells are inside each of the buffers and get the associated groundwater pumping values. The st_join() function from the sf package will do the trick. #--- find irrigation wells inside the buffer and calculate total pumping ---# pumping_neaby &lt;- st_join(buffers, urnrd_gw_sf) Let’s take a look at a USGS monitoring well (site_no = \\(400012101323401\\)). filter(pumping_neaby, site_no == 400012101323401, year == 2010) Simple feature collection with 7 features and 4 fields geometry type: POLYGON dimension: XY bbox: xmin: 279690.7 ymin: 4428006 xmax: 286128 ymax: 4434444 CRS: EPSG:32614 site_no well_id year vol_af geometry 1 400012101323401 6331 2010 NA POLYGON ((286128 4431225, 2... 2 400012101323401 1883 2010 180.189 POLYGON ((286128 4431225, 2... 3 400012101323401 2006 2010 79.201 POLYGON ((286128 4431225, 2... 4 400012101323401 2538 2010 68.205 POLYGON ((286128 4431225, 2... 5 400012101323401 2834 2010 NA POLYGON ((286128 4431225, 2... 6 400012101323401 2834 2010 122.981 POLYGON ((286128 4431225, 2... 7 400012101323401 4983 2010 NA POLYGON ((286128 4431225, 2... As you can see, this well has seven irrigation wells within its 2-mile radius in 2010. Now, we will get total nearby pumping by monitoring well and year. ( total_pumping_nearby &lt;- pumping_neaby %&gt;% #--- calculate total pumping by monitoring well ---# group_by(site_no, year) %&gt;% summarize(nearby_pumping = sum(vol_af, na.rm = TRUE)) %&gt;% #--- NA means 0 pumping ---# mutate( nearby_pumping = ifelse(is.na(nearby_pumping), 0, nearby_pumping) ) ) Simple feature collection with 2396 features and 3 fields geometry type: POLYGON dimension: XY bbox: xmin: 237904.5 ymin: 4428006 xmax: 313476.5 ymax: 4545687 CRS: EPSG:32614 # A tibble: 2,396 x 4 # Groups: site_no [401] site_no year nearby_pumping geometry * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;POLYGON [m]&gt; 1 4000121013… 2007 571. ((286128 4431225, 286123.6 4431057, 286110.… 2 4000121013… 2008 772. ((286128 4431225, 286123.6 4431057, 286110.… 3 4000121013… 2009 500. ((286128 4431225, 286123.6 4431057, 286110.… 4 4000121013… 2010 451. ((286128 4431225, 286123.6 4431057, 286110.… 5 4000121013… 2011 545. ((286128 4431225, 286123.6 4431057, 286110.… 6 4000121013… 2012 1028. ((286128 4431225, 286123.6 4431057, 286110.… 7 4001301013… 2007 485. ((278847.4 4433844, 278843 4433675, 278829.… 8 4001301013… 2008 515. ((278847.4 4433844, 278843 4433675, 278829.… 9 4001301013… 2009 351. ((278847.4 4433844, 278843 4433675, 278829.… 10 4001301013… 2010 374. ((278847.4 4433844, 278843 4433675, 278829.… # … with 2,386 more rows We now merge nearby pumping data to the groundwater level data, and transform the data to obtain the dataset ready for regression analysis. #--- regression-ready data ---# reg_data &lt;- NE_gwl_march %&gt;% #--- pick monitoring wells that are inside the three counties ---# filter(site_no %in% unique(sites_info$site_no)) %&gt;% #--- merge with the nearby pumping data ---# left_join(., total_pumping_nearby, by = c(&quot;site_no&quot;, &quot;year&quot;)) %&gt;% #--- lead depth to water table ---# arrange(site_no, year) %&gt;% group_by(site_no) %&gt;% mutate( #--- lead depth ---# dwt_lead1 = dplyr::lead(dwt, n = 1, default = NA, order_by = year), #--- first order difference in dwt ---# dwt_dif = dwt_lead1 - dwt ) #--- take a look ---# dplyr::select(reg_data, site_no, year, dwt_dif, nearby_pumping) # A tibble: 2,022 x 4 # Groups: site_no [230] site_no year dwt_dif nearby_pumping &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 400130101374401 2011 NA 358. 2 400134101483501 2007 2.87 2038. 3 400134101483501 2008 0.78 2320. 4 400134101483501 2009 -2.45 2096. 5 400134101483501 2010 3.97 2432. 6 400134101483501 2011 1.84 2634. 7 400134101483501 2012 -1.35 985. 8 400134101483501 2013 44.8 NA 9 400134101483501 2014 -26.7 NA 10 400134101483501 2015 NA NA # … with 2,012 more rows Finally, we estimate the model using the lfe package. #--- load the lfe package for regression with fixed effects ---# library(lfe) #--- OLS with site_no and year FEs (error clustered by site_no) ---# reg_dwt &lt;- felm(dwt_dif ~ nearby_pumping | site_no + year | 0 | site_no, data = reg_data) Here is the regression result. stargazer(reg_dwt, type = &quot;html&quot;) Dependent variable: dwt_dif nearby_pumping 0.001*** (0.0001) Observations 1,342 R2 0.409 Adjusted R2 0.286 Residual Std. Error 1.493 (df = 1111) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 the distance from the surface to the top of the aquifer↩ For our geographic focus of southwest Nebraska, corn is the dominant crop type. Irrigation for corn happens typically between April through September. For example, this means that changes in groundwater level (\\(y_{i,2012} - y_{i,2011}\\)) captures the impact of groundwater pumping that occurred April through September in 2011.↩ month() and year() are from the lubridate package. They extract month and year from a Date object.↩ This can alternatively be done using the st_is_within_distance() function.↩ "],
["demonstration-2-precision-agriculture.html", "1.2 Demonstration 2: Precision Agriculture", " 1.2 Demonstration 2: Precision Agriculture .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.2.1 Project Overview Objectives: Understand the impact of nitrogen on corn yield Understand how electric conductivity (EC) affects the marginal impact of nitrogen on corn Datasets: The experimental design of an on-farm randomized nitrogen trail on an 80-acre field Data generated by the experiment As-applied nitrogen rate Yield measures Electric conductivity Econometric Model: Here is the econometric model, we would like to estimate: \\[ yield_i = \\beta_0 + \\beta_1 N_i + \\beta_2 N_i^2 + \\beta_3 N_i \\cdot EC_i + \\beta_4 N_i^2 \\cdot EC_i + v_i \\] where \\(yield_i\\), \\(N_i\\), \\(EC_i\\), and \\(v_i\\) are corn yield, nitrogen rate, EC, and error term at subplot \\(i\\). Subplots which are obtained by dividing experimental plots into six of equal-area compartments. GIS tasks read spatial data in various formats: R data set (rds), shape file, and GeoPackage file use sf::st_read() create maps using the ggplot2 package use ggplot2::geom_sf() create subplots within experimental plots use-defined function that makes use of st_geometry() identify corn yield, as-applied nitrogen, and electric conductivity (EC) data points within each of the experimental plots and find their averages use sf::st_join() and sf::aggregate() Preparation for replication Source (run) Chap_1_Demonstration.R to define theme_map and gen_subplots() source(&quot;Codes/Chap_1_Demonstration.R&quot;) Load (install first if you have not) the following packages (There are other packages that will be loaded during the demonstration). library(sf) library(dplyr) library(ggplot2) library(stargazer) 1.2.2 Project Demonstration We have already run a whole-field randomized nitrogen experiment on a 80-acre field. Let’s import the trial design data #--- read the trial design data ---# trial_design_16 &lt;- readRDS(&quot;./Data/trial_design.rds&quot;) Figure 1.4 is the map of the trial design generated using ggplot2 package.16. #--- map of trial design ---# ggplot(data = trial_design_16) + geom_sf(aes(fill = factor(NRATE))) + scale_fill_brewer(name = &quot;N&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map Figure 1.4: The Experimental Design of the Randomize Nitrogen Trial We have collected yield, as-applied NH3, and EC data. Let’s read in these datasets:17 #--- read yield data (sf data saved as rds) ---# yield &lt;- readRDS(&quot;./Data/yield.rds&quot;) #--- read NH3 data (GeoPackage data) ---# NH3_data &lt;- st_read(&quot;Data/NH3.gpkg&quot;) #--- read ec data (shape file) ---# ec &lt;- st_read(dsn=&quot;Data&quot;, &quot;ec&quot;) Figure 1.5 shows the spatial distribution of the three variables. A map of each variable was made first, and then they are combined into one figure using the patchwork package18. #--- yield map ---# g_yield &lt;- ggplot() + geom_sf(data = trial_design_16) + geom_sf(data = yield, aes(color = yield), size = 0.5) + scale_color_distiller(name = &quot;Yield&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map #--- NH3 map ---# g_NH3 &lt;- ggplot() + geom_sf(data = trial_design_16) + geom_sf(data = NH3_data, aes(color = aa_NH3), size = 0.5) + scale_color_distiller(name = &quot;NH3&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map #--- NH3 map ---# g_ec &lt;- ggplot() + geom_sf(data = trial_design_16) + geom_sf(data = ec, aes(color = ec), size = 0.5) + scale_color_distiller(name = &quot;EC&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map #--- stack the figures vertically and display ---# library(patchwork) g_yield/g_NH3/g_ec Figure 1.5: Spatial distribution of yield, NH3, and EC Instead of using plot as the observation unit, we would like to create subplots inside each of the plots and make them the unit of analysis because it would avoid masking the within-plot spatial heterogeneity of EC. Here, we divide each plot into six subplots19: #--- generate subplots ---# subplots &lt;- lapply( 1:nrow(trial_design_16), function(x) gen_subplots(trial_design_16[x, ], 6) ) %&gt;% do.call(&#39;rbind&#39;, .) Figure 1.6 is a map of the subplots generated. #--- here is what subplots look like ---# ggplot(subplots) + geom_sf() + theme_for_map Figure 1.6: Map of the subplots We now identify the mean value of corn yield, nitrogen rate, and EC for each of the subplots using sf::aggregate() and sf::st_join(). ( reg_data &lt;- subplots %&gt;% #--- yield ---# st_join(., aggregate(yield, ., mean), join = st_equals) %&gt;% #--- nitrogen ---# st_join(., aggregate(NH3_data, ., mean), join = st_equals) %&gt;% #--- EC ---# st_join(., aggregate(ec, ., mean), join = st_equals) ) Simple feature collection with 816 features and 3 fields geometry type: POLYGON dimension: XY bbox: xmin: 560121.3 ymin: 4533410 xmax: 560758.9 ymax: 4533734 CRS: EPSG:26914 First 10 features: yield aa_NH3 ec geometry 1 220.1789 194.5155 28.33750 POLYGON ((560121.3 4533428,... 2 218.9671 194.4291 29.37667 POLYGON ((560134.5 4533428,... 3 220.3286 195.2903 30.73600 POLYGON ((560147.7 4533428,... 4 215.3121 196.7649 32.24000 POLYGON ((560160.9 4533429,... 5 216.9709 195.2199 36.27000 POLYGON ((560174.1 4533429,... 6 227.8761 184.6362 31.21000 POLYGON ((560187.3 4533429,... 7 226.0991 179.2143 31.99250 POLYGON ((560200.5 4533430,... 8 225.3973 179.0916 31.56500 POLYGON ((560213.7 4533430,... 9 221.1820 178.9585 33.01000 POLYGON ((560227 4533430, 5... 10 219.4659 179.0057 41.89750 POLYGON ((560240.2 4533430,... Here are the visualization of the subplot-level data (Figure 1.7): (ggplot() + geom_sf(data = reg_data, aes(fill = yield), color = NA) + scale_fill_distiller(name = &quot;Yield&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map)/ (ggplot() + geom_sf(data = reg_data, aes(fill = aa_NH3), color = NA) + scale_fill_distiller(name = &quot;NH3&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map)/ (ggplot() + geom_sf(data = reg_data, aes(fill = ec), color = NA) + scale_fill_distiller(name = &quot;EC&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map) Figure 1.7: Spatial distribution of subplot-level yield, NH3, and EC Let’s estimate the model and see the results: lm(yield ~ aa_NH3 + I(aa_NH3^2) + I(aa_NH3*ec) + I(aa_NH3^2*ec), data = reg_data) %&gt;% stargazer(type = &quot;html&quot;) Dependent variable: yield aa_NH3 -1.223 (1.308) I(aa_NH32) 0.004 (0.003) I(aa_NH3 * ec) 0.002 (0.003) I(aa_NH32 * ec) -0.00001 (0.00002) Constant 327.993*** (125.638) Observations 784 R2 0.010 Adjusted R2 0.005 Residual Std. Error 5.712 (df = 779) F Statistic 2.023* (df = 4; 779) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 theme_for_map is a user defined object that defines the theme of figures generated using ggplot2 for this section. You can find it in Chap_1_Demonstration.R.↩ Here we are demonstrating that R can read spatial data in different formats. R can read spatial data of many other formats. Here, we are reading a shapefile (.shp) and GeoPackage file (.gpkg).↩ Here is its github page. See the bottom of the page to find vignettes.↩ gen_subplots is a user-defined function. See Chap_1_Demonstration.R.↩ "],
["demonstration-3-land-use-and-weather.html", "1.3 Demonstration 3: Land Use and Weather", " 1.3 Demonstration 3: Land Use and Weather .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.3.1 Project Overview Objective Understand the impact of past precipitation on crop choice in Iowa (IA). Datasets IA county boundary Regular grids over IA, created using sf::st_make_grid() PRISM daily precipitation data downloaded using prism package Land use data from the Cropland Data Layer (CDL) for IA in 2015, downloaded using cdlTools package Econometric Model The econometric model we would like to estimate is: \\[ CS_i = \\alpha + \\beta_1 PrN_{i} + \\beta_2 PrC_{i} + v_i \\] where \\(CS_i\\) is the area share of corn divided by that of soy in 2015 for grid \\(i\\) (we will generate regularly-sized grids in the Demo section), \\(PrN_i\\) is the total precipitation observed in April through May and September in 2014, \\(PrC_i\\) is the total precipitation observed in June through August in 2014, and \\(v_i\\) is the error term. To run the econometric model, we need to find crop share and weather variables observed at the grids. We first tackle the crop share variable, and then the precipitation variable. GIS tasks download Cropland Data Layer (CDL) data by USDA NASS use cdlTools::getCDL() download PRISM weather data use prism::get_prism_dailys() crop PRISM data to the geographic extent of IA use raster::crop() create regular grids within IA, which become the observation units of the econometric analysis use sf::st_make_grid() remove grids that share small area with IA use sf::st_intersection() and sf::st_area assign crop share and weather data to each of the generated IA grids (parallelized) use exactextractr::exact_extract() and future.apply::future_lapply() create maps use tmap package Preparation for replication Load (install first if you have not) the following packages (There are other packages that will be loaded during the demonstration). library(sf) library(data.table) library(dplyr) library(raster) library(lubridate) library(tmap) library(future.apply) library(stargazer) 1.3.2 Project Demonstration The geographic focus of this project is IA. Let’s get IAs state border (see Figure 1.8 for its map). library(&quot;maps&quot;) #--- IA state boundary ---# IA_boundary &lt;- st_as_sf(map(&quot;state&quot;, &quot;iowa&quot;, plot = FALSE, fill = TRUE)) Figure 1.8: IA state boundary The unit of analysis is artificial grids that we create over IA. The grids are regularly-sized rectangles except around the edge of the IA state border20. So, let’s create grids and remove those that do not overlap much with IA. #--- create regular grids (40 cells by 40 columns) over IA ---# IA_grids &lt;- IA_boundary %&gt;% #--- create grids ---# st_make_grid(, n = c(40, 40)) %&gt;% #--- convert to sf ---# st_as_sf() %&gt;% #--- find the intersections of IA grids and IA polygon ---# st_intersection(., IA_boundary) %&gt;% #--- calculate the area of each grid ---# mutate( area = as.numeric(st_area(.)), area_ratio = area/max(area) ) %&gt;% #--- keep only if the intersected area is large enough ---# filter(area_ratio &gt; 0.8) %&gt;% #--- assign grid id for future merge ---# mutate(grid_id = 1:nrow(.)) Here is what the generated grids look like (Figure 1.9): #--- plot the grids over the IA state border ---# tm_shape(IA_boundary) + tm_polygons(col = &quot;green&quot;) + tm_shape(IA_grids) + tm_polygons(alpha = 0) + tm_layout(frame = FALSE) Figure 1.9: Map of regular grids generated over IA Let’s work on crop share data. You can download CDL data using the getCDL() function from the cdlTools package. #--- load the cdlTools package ---# library(cdlTools) #--- download the CDL data for IA in 2015 ---# ( IA_cdl_2015 &lt;- getCDL(&quot;Iowa&quot;, 2015)$IA2015 ) class : RasterLayer dimensions : 11671, 17795, 207685445 (nrow, ncol, ncell) resolution : 30, 30 (x, y) extent : -52095, 481755, 1938165, 2288295 (xmin, xmax, ymin, ymax) crs : +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs source : /private/var/folders/t4/5gnqprbn38nftyxkyk5hdwmd8hnypy/T/RtmpaLQfoX/CDL_2015_19.tif names : CDL_2015_19 values : 0, 255 (min, max) The cells (30 meter by 30 meter) of the imported raster layer take a value ranging from 0 to 255. Corn and soybean are represented by 1 and 5, respectively (visualization of the CDL data is on the right). Figure 1.10 shows the map of one of the IA grids and the CDL cells it overlaps with. Figure 1.10: Spatial overlap of a IA grid and CDL layer We would like to extract all the cell values within the blue border. We use exactextractr::exact_extract() to identify which cells of the CDL raster layer fall within each of the IA grids and extract land use type values. We then find the share of corn and soybean for each of the grids. #--- reproject grids to the CRS of the CDL data ---# IA_grids_rp_cdl &lt;- st_transform(IA_grids, projection(IA_cdl_2015)) #--- load the exactextractr package for fast rater value extractions for polygons ---# library(exactextractr) #--- extract crop type values and find frequencies ---# cdl_extracted &lt;- exact_extract(IA_cdl_2015, IA_grids_rp_cdl) %&gt;% lapply(., function (x) data.table(x)[,.N, by = value]) %&gt;% #--- combine the list of data.tables into one data.table ---# rbindlist(idcol = TRUE) %&gt;% #--- find the share of each land use type ---# .[, share := N/sum(N), by = .id] %&gt;% .[, N := NULL] %&gt;% #--- keep only the share of corn and soy ---# .[value %in% c(1, 5), ] We then find the corn to soy ratio for each of the IA grids. #--- find corn/soy ratio ---# corn_soy &lt;- cdl_extracted %&gt;% #--- long to wide ---# dcast(.id ~ value, value.var = &quot;share&quot;) %&gt;% #--- change variable names ---# setnames(c(&quot;.id&quot;, &quot;1&quot;, &quot;5&quot;), c(&quot;grid_id&quot;, &quot;corn_share&quot;, &quot;soy_share&quot;)) %&gt;% #--- corn share divided by soy share ---# .[, c_s_ratio := corn_share / soy_share] We are still missing daily precipitation data at the moment. We have decided to use daily weather data from PRISM. Daily PRISM data is a raster data with the cell size of 4 km by 4 km. Figure 1.11 the right presents precipitation data downloaded for April 1, 2010. It covers the entire contiguous U.S. Figure 1.11: Map of PRISM raster data layer Let’s now download PRISM data21. This can be done using the get_prism_dailys() function from the prism package.22 options(prism.path = &quot;./Data/PRISM&quot;) get_prism_dailys( type = &quot;ppt&quot;, minDate = &quot;2014-04-01&quot;, maxDate = &quot;2014-09-30&quot;, keepZip = FALSE ) When we use get_prism_dailys() to download data23, it creates one folder for each day. So, I have about 180 folders inside the folder I designated as the download destination above with the options() function. We now try to extract precipitation value by day for each of the IA grids by geographically overlaying IA grids onto the PRISM data layer and identify which PRISM cells each of the IA grid encompass. Figure 1.12 shows how the first IA grid overlaps with the PRISM cells24. #--- read a PRISM dataset ---# prism_whole &lt;- raster(&quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140401_bil/PRISM_ppt_stable_4kmD2_20140401_bil.bil&quot;) #--- align the CRS ---# IA_grids_rp_prism &lt;- st_transform(IA_grids, projection(prism_whole)) #--- crop the PRISM data for the 1st IA grid ---# PRISM_1 &lt;- crop(prism_whole, st_buffer(IA_grids_rp_prism[1, ], dist = 0.05)) #--- map them ---# tm_shape(PRISM_1) + tm_raster() + tm_shape(IA_grids_rp_prism[1, ]) + tm_polygons(alpha = 0) + tm_layout(frame = NA) Figure 1.12: Spatial overlap of an IA grid over PRISM cells As you can see, some PRISM grids are fully inside the analysis grid, while others are partially inside it. So, when assigning precipitation values to grids, we will use the coverage-weighted mean of precipitations25. Unlike the CDL layer, we have 183 raster layers to process. Fortunately, we can process many raster files at the same time very quickly by first “stacking” many raster files first and then applying the exact_extract() function. Using future_lapply(), we let \\(6\\) cores take care of this task with each processing 31 files, except one of them handling only 28 files.26 We first get all the paths to the PRISM files. #--- get all the dates ---# dates_ls &lt;- seq(as.Date(&quot;2014-04-01&quot;), as.Date(&quot;2014-09-30&quot;), &quot;days&quot;) #--- remove hyphen ---# dates_ls_no_hyphen &lt;- str_remove_all(dates_ls, &quot;-&quot;) #--- get all the prism file names ---# folder_name &lt;- paste0(&quot;PRISM_ppt_stable_4kmD2_&quot;, dates_ls_no_hyphen, &quot;_bil&quot;) file_name &lt;- paste0(&quot;PRISM_ppt_stable_4kmD2_&quot;, dates_ls_no_hyphen, &quot;_bil.bil&quot;) file_paths &lt;- paste0(&quot;./Data/PRISM/&quot;, folder_name, &quot;/&quot;, file_name) #--- take a look ---# head(file_paths) [1] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140401_bil/PRISM_ppt_stable_4kmD2_20140401_bil.bil&quot; [2] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140402_bil/PRISM_ppt_stable_4kmD2_20140402_bil.bil&quot; [3] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140403_bil/PRISM_ppt_stable_4kmD2_20140403_bil.bil&quot; [4] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140404_bil/PRISM_ppt_stable_4kmD2_20140404_bil.bil&quot; [5] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140405_bil/PRISM_ppt_stable_4kmD2_20140405_bil.bil&quot; [6] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140406_bil/PRISM_ppt_stable_4kmD2_20140406_bil.bil&quot; We now prepare for parallelized extractions and then implement them using future_apply(). #--- define the number of cores to use ---# num_core &lt;- 6 #--- prepare some parameters for parallelization ---# file_len &lt;- length(file_paths) files_per_core &lt;- ceiling(file_len/num_core) #--- prepare for parallel processing ---# plan(multiprocess, workers = num_core) #--- reproject IA grids to the CRS of PRISM data ---# IA_grids_reprojected &lt;- st_transform(IA_grids, projection(prism_whole)) Here is the function that we run in parallel over 6 cores. #--- define the function to extract PRISM values by block of files ---# extract_by_block &lt;- function(i, files_per_core) { #--- files processed by core ---# start_file_index &lt;- (i-1) * files_per_core + 1 #--- indexes for files to process ---# file_index &lt;- seq( from = start_file_index, to = min((start_file_index + files_per_core), file_len), by = 1 ) #--- extract values ---# data_temp &lt;- file_paths[file_index] %&gt;% # get file names #--- stack files ---# stack() %&gt;% #--- extract ---# exact_extract(., IA_grids_reprojected) %&gt;% #--- combine into one data set ---# rbindlist(idcol = &quot;ID&quot;) %&gt;% #--- wide to long ---# melt(id.var = c(&quot;ID&quot;, &quot;coverage_fraction&quot;)) %&gt;% #--- calculate &quot;area&quot;-weighted mean ---# .[, .(value = sum(value * coverage_fraction)/sum(coverage_fraction)), by = .(ID, variable)] return(data_temp) } Now, let’s run the function in parallel and calculate precipitation by period. #--- run the function ---# precip_by_period &lt;- future_lapply(1:num_core, function(x) extract_by_block(x, files_per_core)) %&gt;% rbindlist() %&gt;% #--- recover the date ---# .[, variable := as.Date(str_extract(variable, &quot;[0-9]{8}&quot;), &quot;%Y%m%d&quot;)] %&gt;% #--- change the variable name to date ---# setnames(&quot;variable&quot;, &quot;date&quot;) %&gt;% #--- define critical period ---# .[,critical := &quot;non_critical&quot;] %&gt;% .[month(date) %in% 6:8, critical := &quot;critical&quot;] %&gt;% #--- total precipitation by critical dummy ---# .[, .(precip=sum(value)), by = .(ID, critical)] %&gt;% #--- wide to long ---# dcast(ID ~ critical, value.var = &quot;precip&quot;) We now have grid-level crop share and precipitation data. Let’s merge them and run regression.27 #--- crop share ---# reg_data &lt;- corn_soy[precip_by_period, on = c(grid_id = &quot;ID&quot;)] #--- OLS ---# reg_results &lt;- lm(c_s_ratio ~ critical + non_critical, data = reg_data) Here is the regression results table. #--- regression table ---# stargazer(reg_results, type = &quot;html&quot;) Dependent variable: c_s_ratio critical -0.002*** (0.0003) non_critical -0.0003 (0.0003) Constant 2.701*** (0.161) Observations 1,218 R2 0.058 Adjusted R2 0.056 Residual Std. Error 0.743 (df = 1215) F Statistic 37.234*** (df = 2; 1215) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Again, do not read into the results as the econometric model is terrible. We by no means are saying that this is the right geographical unit of analysis. This is just about demonstrating how R can be used for analysis done at the higher spatial resolution than county.↩ You do not have to run this code to download the data. It is included in the data folder for replication (here).↩ prism github page↩ For this project, I could have just used monthly PRISM data, which can be downloaded using the get_prism_monthlys() function. But, in many applications, daily data is necessary, so I wanted to illustrate how to download and process them.↩ Do not use st_buffer() for spatial objects in geographic coordinates (latitude, longitude) if you intend to use the created buffers for any serious IA (it is difficult to get the right distance parameter anyway.). Significant distortion will be introduced to the buffer due to the fact that one degree in latitude and longitude means different distances at the latitude of IA. Here, I am just creating a buffer to extract PRISM cells to display on the map.↩ In practice, this may not be advisable. The coverage fraction calculation by exact_extract() is done using latitude and longitude. Therefore, the relative magnitude of the fraction numbers incorrectly reflects the actual relative magnitude of the overlapped area. When the spatial resolution of the sources grids (grids from which you extract values) is much smaller relative to that of the target grids (grids to which you assign values to), then a simple average would be very similar to a coverage-weighted mean. For example, CDL consists of 30m by 30m grids, and more than \\(1,000\\) grids are inside one analysis grid.↩ Parallelization of extracting values from many raster layers for polygons are discussed in much more detail in Chapter ??. When I tried stacking all 183 files into one stack and applying exact_extract, it did not finish the job after over five minutes. So, I terminated the process in the middle. The parallelized version gets the job done in about \\(30\\) seconds on my desktop.↩ We can match on grid_id from corn_soy and ID from “precip_by_period” because grid_id is identical with the row number and ID variables were created so that the ID value of \\(i\\) corresponds to \\(i\\) th row of IA_grids.↩ "],
["demonstration-4-the-impact-of-railroad-presence-on-corn-planted-acreage.html", "1.4 Demonstration 4: The Impact of Railroad Presence on Corn Planted Acreage", " 1.4 Demonstration 4: The Impact of Railroad Presence on Corn Planted Acreage .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.4.1 Project Overview Objective Understand the impact of railroad on corn planted acreage in Illinois Datasets USDA corn planted acreage for Illinois downloaded from the USDA NationalAgricultural Statistics Service (NASS) QuickStats service using tidyUSDA package US railroads (line data) downloaded from here Econometric Model We will estimate the following model: \\[ y_i = \\beta_0 + \\beta_1 RL_i + v_i \\] where \\(y_i\\) is corn planted acreage in county \\(i\\) in Illinois, \\(RL_i\\) is the total length of railroad, and \\(v_i\\) is the error term. GIS tasks Download USDA corn planted acreage by county as a spatial dataset (sf object) use tidyUSDA::getQuickStat() Import US railroad shape file as a spatial dataset (sf object) use sf:st_read() Spatially subset (crop) the railroad data to the geographic boundary of Illinois use sf_1[sf_2, ] Find railroads for each county (cross-county railroad will be chopped into pieces for them to fit within a single county) use sf::st_intersection() Calculate the travel distance of each railroad piece use sf::st_length() Preparation for replication Load (install first if you have not) the following packages (There are other packages that will be loaded during the demonstration). library(sf) library(ggplot2) library(dplyr) library(stargazer) 1.4.2 Project Demonstration We first download corn planted acreage data for 2018 from USDA NASS QuickStat service using tidyUSDA package28. library(keyring) library(tidyUSDA) ( IL_corn_planted &lt;- getQuickstat( key = key_get(&quot;usda_nass_qs_api&quot;) , program = &quot;SURVEY&quot;, data_item = &quot;CORN - ACRES PLANTED&quot;, geographic_level = &quot;COUNTY&quot;, state = &quot;ILLINOIS&quot;, year = &quot;2018&quot;, geometry = TRUE ) %&gt;% #--- keep only some of the variables ---# dplyr::select(year, NAME, county_code, short_desc, Value) ) Simple feature collection with 90 features and 5 fields (with 6 geometries empty) geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -91.51308 ymin: 36.9703 xmax: -87.4952 ymax: 42.50848 CRS: +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs First 10 features: year NAME county_code short_desc Value 1 2018 Bureau 011 CORN - ACRES PLANTED 264000 2 2018 Carroll 015 CORN - ACRES PLANTED 134000 3 2018 Henry 073 CORN - ACRES PLANTED 226500 4 2018 Jo Daviess 085 CORN - ACRES PLANTED 98500 5 2018 Lee 103 CORN - ACRES PLANTED 236500 6 2018 Mercer 131 CORN - ACRES PLANTED 141000 7 2018 Ogle 141 CORN - ACRES PLANTED 217000 8 2018 Putnam 155 CORN - ACRES PLANTED 32300 9 2018 Rock Island 161 CORN - ACRES PLANTED 68400 10 2018 Stephenson 177 CORN - ACRES PLANTED 166500 geometry 1 MULTIPOLYGON (((-89.8569 41... 2 MULTIPOLYGON (((-90.16133 4... 3 MULTIPOLYGON (((-90.43227 4... 4 MULTIPOLYGON (((-90.50668 4... 5 MULTIPOLYGON (((-89.63118 4... 6 MULTIPOLYGON (((-90.99255 4... 7 MULTIPOLYGON (((-89.68598 4... 8 MULTIPOLYGON (((-89.33303 4... 9 MULTIPOLYGON (((-90.33573 4... 10 MULTIPOLYGON (((-89.9205 42... A nice thing about this function is that the data is downloaded as an sf object with county geometry with geometry = TRUE. So, you can immediately plot it (Figure 1.13) and use it for later spatial interactions without having to merge the downloaded data to an independent county boundary data.29. ggplot(IL_corn_planted) + geom_sf(aes(fill = Value/1000)) + scale_fill_distiller(name = &quot;Planted Acreage (1000 acres)&quot;, palette = &quot;YlOrRd&quot;, trans = &quot;reverse&quot;) + theme( legend.position = &quot;bottom&quot; ) + theme_for_map Figure 1.13: Map of Con Planted Acreage in Illinois in 2018 Let’s import the U.S. railroad data and reproject to the CRS of IL_corn_planted: rail_roads &lt;- st_read(dsn = &quot;./Data/&quot;, layer = &quot;tl_2015_us_rails&quot;) %&gt;% st_transform(st_crs(IL_corn_planted)) Reading layer `tl_2015_us_rails&#39; from data source `/Users/tmieno2/Box/Teaching/AAEA R/GIS/Data&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 180958 features and 3 fields geometry type: MULTILINESTRING dimension: XY bbox: xmin: -165.4011 ymin: 17.95174 xmax: -65.74931 ymax: 65.00006 CRS: 4269 Here is what it looks like: ggplot(rail_roads) + geom_sf() + theme_for_map Figure 1.14: Map of Railroads We now crop it to the Illinois state border (Figure 1.15) using sf_1[sf_2, ]: rail_roads_IL &lt;- rail_roads[IL_corn_planted, ] ggplot() + geom_sf(data = rail_roads_IL) + theme_for_map Figure 1.15: Map of railroads in Illinois Let’s now find railroads for each county, where cross-county railroads will be chopped into pieces so each piece fits completely within a single county, using st_intersection(). rails_IL_segmented &lt;- st_intersection(rail_roads_IL, IL_corn_planted) Here are the railroads for Richland County: ggplot() + geom_sf(data = dplyr::filter(IL_corn_planted, NAME == &quot;Richland&quot;)) + geom_sf(data = dplyr::filter(rails_IL_segmented, NAME == &quot;Richland&quot;), aes( color = LINEARID )) + theme( legend.position = &quot;bottom&quot; ) + theme_for_map We now calculate the travel distance (Great-circle distance) of each railroad piece using st_length() and then sum them up by county to find total railroad length by county. ( rail_length_county &lt;- mutate( rails_IL_segmented, length_in_m = as.numeric(st_length(rails_IL_segmented)), ) %&gt;% #--- group by county ID ---# group_by(county_code) %&gt;% #--- sum rail length by county ---# summarize(length_in_m = sum(length_in_m)) %&gt;% #--- geometry no longer needed ---# st_drop_geometry() ) # A tibble: 82 x 2 county_code length_in_m * &lt;chr&gt; &lt;dbl&gt; 1 001 77221. 2 003 77290. 3 007 36764. 4 011 255441. 5 015 161726. 6 017 30585. 7 019 389226. 8 021 155794. 9 023 78587. 10 025 92030. # … with 72 more rows We merge the railroad length data to the corn planted acreage data and estimate the model. reg_data &lt;- left_join(IL_corn_planted, rail_length_county, by = &quot;county_code&quot;) lm(Value ~ length_in_m, data = reg_data) %&gt;% stargazer(type = &quot;html&quot;) Dependent variable: Value length_in_m 0.092* (0.047) Constant 108,154.800*** (11,418.900) Observations 82 R2 0.046 Adjusted R2 0.034 Residual Std. Error 69,040.680 (df = 80) F Statistic 3.866* (df = 1; 80) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 In order to actually download the data, you need to obtain the API key here. Once the API key was obtained, I stored it using set_key() from the keyring package, which was named “usda_nass_qs_api”. In the code to the left, I retrieve the API key using key_get(&quot;usda_nass_qs_api&quot;) in the code.↩ theme_for_map is a user defined object that defines the theme of figures generated using ggplot2 for this section. You can find it in Chap_1_Demonstration.R.↩ "],
["demonstration-5-groundwater-use-for-agricultural-irrigation.html", "1.5 Demonstration 5: Groundwater use for agricultural irrigation", " 1.5 Demonstration 5: Groundwater use for agricultural irrigation .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.5.1 Project Overview Objective + Understand the impact of monthly precipitation on groundwater use for agricultural irrigation Datasets Annual groundwater pumping by irrigation wells in Kansas for 2010 and 2011 (originally obtained from the Water Information Management &amp; Analysis System (WIMAS) database) Daymet30 daily precipitation and maximum temperature downloaded using daymetr package Econometric Model The econometric model we would like to estimate is: \\[ y_{i,t} = \\alpha + P_{i,t} \\beta + T_{i,t} \\gamma + \\phi_i + \\eta_t + v_{i,t} \\] where \\(y\\) is the total groundwater extracted in year \\(t\\), \\(P_{i,t}\\) and \\(T_{i,t}\\) is the collection of monthly total precipitation and mean maximum temperature April through September in year \\(t\\), respectively, \\(\\phi_i\\) is the well fixed effect, \\(\\eta_t\\) is the year fixed effect, and \\(v_{i,t}\\) is the error term. GIS tasks download Daymet precipitation and maximum temperature data for each well from within R in parallel use daymetr::download_daymet() and future.apply::future_lapply() 1.5.2 Project Demonstration We have already collected annual groundwater pumping data by irrigation wells in 2010 and 2011 in Kansas from the Water Information Management &amp; Analysis System (WIMAS) database. Let’s read in the groundwater use data. #--- read in the data ---# ( gw_KS_sf &lt;- readRDS( &quot;./Data/gw_KS_sf.rds&quot;) ) Simple feature collection with 56225 features and 3 fields geometry type: POINT dimension: XY bbox: xmin: -102.0495 ymin: 36.99561 xmax: -94.70746 ymax: 40.00191 CRS: EPSG:4269 First 10 features: well_id year af_used geometry 1 1 2010 67.00000 POINT (-100.4423 37.52046) 2 1 2011 171.00000 POINT (-100.4423 37.52046) 3 3 2010 30.93438 POINT (-100.7118 39.91526) 4 3 2011 12.00000 POINT (-100.7118 39.91526) 5 7 2010 0.00000 POINT (-101.8995 38.78077) 6 7 2011 0.00000 POINT (-101.8995 38.78077) 7 11 2010 154.00000 POINT (-101.7114 39.55035) 8 11 2011 160.00000 POINT (-101.7114 39.55035) 9 12 2010 28.17239 POINT (-95.97031 39.16121) 10 12 2011 89.53479 POINT (-95.97031 39.16121) We have 28553 wells in total, and each well has records of groundwater pumping (af_used) for years 2010 and 2011. Here is the spatial distribution of the wells. We now need to get monthly precipitation and maximum temperature data. We have decided that we use Daymet weather data. Here we use the download_daymet() function from the daymetr package31 that allows us to download all the weather variables for a specified geographic location and time period32. We write a wrapper function that downloads Daymet data and then processes it to find monthly total precipitation and mean maximum temperature33. We then loop over the 56225 wells, which is parallelized using the future_apply() function34 from the future.apply package. This process takes about an hour on my Mac with parallelization on 7 cores. The data is available in the data repository for this course (named as “all_daymet.rds”). library(daymetr) library(future.apply) #--- get the geographic coordinates of the wells ---# well_locations &lt;- gw_KS_sf %&gt;% unique(by = &quot;well_id&quot;) %&gt;% dplyr::select(well_id) %&gt;% cbind(., st_coordinates(.)) #--- define a function that downloads Daymet data by well and process it ---# get_daymet &lt;- function(i) { temp_site &lt;- well_locations[i, ]$well_id temp_long &lt;- well_locations[i, ]$X temp_lat &lt;- well_locations[i, ]$Y data_temp &lt;- download_daymet( site = temp_site, lat = temp_lat, lon = temp_long, start = 2010, end = 2011, #--- if TRUE, tidy data is returned ---# simplify = TRUE, #--- if TRUE, the downloaded data can be assigned to an R object ---# internal = TRUE ) %&gt;% data.table() %&gt;% #--- keep only precip and tmax ---# .[measurement %in% c(&quot;prcp..mm.day.&quot;, &quot;tmax..deg.c.&quot;), ] %&gt;% #--- recover calender date from Julian day ---# .[, date := as.Date(paste(year, yday, sep = &quot;-&quot;), &quot;%Y-%j&quot;)] %&gt;% #--- get month ---# .[, month := month(date)] %&gt;% #--- keep only April through September ---# .[month %in% 4:9,] %&gt;% .[, .(site, year, month, date, measurement, value)] %&gt;% #--- long to wide ---# dcast(site + year + month + date~ measurement, value.var = &quot;value&quot;) %&gt;% #--- change variable names ---# setnames(c(&quot;prcp..mm.day.&quot;, &quot;tmax..deg.c.&quot;), c(&quot;prcp&quot;, &quot;tmax&quot;)) %&gt;% #--- find the total precip and mean tmax by month-year ---# .[, .(prcp = sum(prcp), tmax = mean(tmax)) , by = .(month, year)] %&gt;% .[, well_id := temp_site] return(data_temp) gc() } Here is what one run (for the first well) of get_daymet() returns #--- one run ---# ( returned_data &lt;- get_daymet(1)[] ) month year prcp tmax well_id 1: 4 2010 42 20.96667 1 2: 5 2010 94 24.19355 1 3: 6 2010 70 32.51667 1 4: 7 2010 89 33.50000 1 5: 8 2010 63 34.17742 1 6: 9 2010 15 31.43333 1 7: 4 2011 25 21.91667 1 8: 5 2011 26 26.30645 1 9: 6 2011 23 35.16667 1 10: 7 2011 35 38.62903 1 11: 8 2011 37 36.90323 1 12: 9 2011 9 28.66667 1 We get the number of cores you can use by RhpcBLASctl::get_num_procs() and parallelize the loop over wells using future_lapply().35 #--- prepare parallelized process ---# library(RhpcBLASctl) num_core &lt;- get_num_procs() - 1 #--- run get_daymet with parallelization ---# ( all_daymet &lt;- future_lapply(1:nrow(well_locations), get_daymet) %&gt;% rbindlist() ) month year prcp tmax well_id 1: 4 2010 42 20.96667 1 2: 5 2010 94 24.19355 1 3: 6 2010 70 32.51667 1 4: 7 2010 89 33.50000 1 5: 8 2010 63 34.17742 1 --- 336980: 5 2011 18 26.11290 78051 336981: 6 2011 25 34.61667 78051 336982: 7 2011 6 38.37097 78051 336983: 8 2011 39 36.66129 78051 336984: 9 2011 23 28.45000 78051 Before merging the Daymet data, we need to reshape the data into a wide format to get monthly precipitation and maximum temperature as columns. #--- long to wide ---# daymet_to_merge &lt;- dcast(all_daymet, well_id + year ~ month, value.var = c(&quot;prcp&quot;, &quot;tmax&quot;)) #--- take a look ---# daymet_to_merge well_id year prcp_4 prcp_5 prcp_6 prcp_7 prcp_8 prcp_9 tmax_4 tmax_5 1: 1 2010 42 94 70 89 63 15 20.96667 24.19355 2: 1 2011 25 26 23 35 37 9 21.91667 26.30645 3: 3 2010 85 62 109 112 83 41 19.93333 21.64516 4: 3 2011 80 104 44 124 118 14 18.40000 22.62903 5: 7 2010 44 83 23 99 105 13 18.81667 22.14516 --- 56160: 78049 2011 27 6 38 37 34 36 22.81667 26.70968 56161: 78050 2010 35 48 68 111 56 9 21.38333 24.85484 56162: 78050 2011 26 7 44 38 34 35 22.76667 26.70968 56163: 78051 2010 30 62 48 29 76 3 21.05000 24.14516 56164: 78051 2011 33 18 25 6 39 23 21.90000 26.11290 tmax_6 tmax_7 tmax_8 tmax_9 1: 32.51667 33.50000 34.17742 31.43333 2: 35.16667 38.62903 36.90323 28.66667 3: 30.73333 32.80645 33.56452 28.93333 4: 30.08333 35.08065 32.90323 25.81667 5: 31.30000 33.12903 32.67742 30.16667 --- 56160: 35.01667 38.32258 36.54839 28.80000 56161: 33.16667 33.88710 34.40323 32.11667 56162: 34.91667 38.32258 36.54839 28.83333 56163: 32.90000 33.83871 34.38710 31.56667 56164: 34.61667 38.37097 36.66129 28.45000 Now, let’s merge the weather data to the groundwater pumping dataset. ( reg_data &lt;- data.table(gw_KS_sf) %&gt;% #--- keep only the relevant variables ---# .[, .(well_id, year, af_used)] %&gt;% #--- join ---# daymet_to_merge[., on = c(&quot;well_id&quot;, &quot;year&quot;)] ) well_id year prcp_4 prcp_5 prcp_6 prcp_7 prcp_8 prcp_9 tmax_4 tmax_5 1: 1 2010 42 94 70 89 63 15 20.96667 24.19355 2: 1 2011 25 26 23 35 37 9 21.91667 26.30645 3: 3 2010 85 62 109 112 83 41 19.93333 21.64516 4: 3 2011 80 104 44 124 118 14 18.40000 22.62903 5: 7 2010 44 83 23 99 105 13 18.81667 22.14516 --- 56221: 79348 2011 NA NA NA NA NA NA NA NA 56222: 79349 2011 NA NA NA NA NA NA NA NA 56223: 79367 2011 NA NA NA NA NA NA NA NA 56224: 79372 2011 NA NA NA NA NA NA NA NA 56225: 80930 2011 NA NA NA NA NA NA NA NA tmax_6 tmax_7 tmax_8 tmax_9 af_used 1: 32.51667 33.50000 34.17742 31.43333 67.00000 2: 35.16667 38.62903 36.90323 28.66667 171.00000 3: 30.73333 32.80645 33.56452 28.93333 30.93438 4: 30.08333 35.08065 32.90323 25.81667 12.00000 5: 31.30000 33.12903 32.67742 30.16667 0.00000 --- 56221: NA NA NA NA 76.00000 56222: NA NA NA NA 182.00000 56223: NA NA NA NA 0.00000 56224: NA NA NA NA 134.00000 56225: NA NA NA NA 23.69150 Let’s run regression and display the results. #--- load lfe package ---# library(lfe) #--- run FE ---# reg_results &lt;- felm( af_used ~ prcp_4 + prcp_5 + prcp_6 + prcp_7 + prcp_8 + prcp_9 + tmax_4 + tmax_5 + tmax_6 + tmax_7 + tmax_8 + tmax_9 |well_id + year| 0 | well_id, data = reg_data ) #--- display regression results ---# stargazer(reg_results, type = &quot;html&quot;) Dependent variable: af_used prcp_4 -0.053*** (0.017) prcp_5 0.112*** (0.010) prcp_6 -0.073*** (0.008) prcp_7 0.014 (0.010) prcp_8 0.093*** (0.014) prcp_9 -0.177*** (0.025) tmax_4 9.159*** (1.227) tmax_5 -7.505*** (1.062) tmax_6 15.134*** (1.360) tmax_7 3.969** (1.618) tmax_8 3.420*** (1.066) tmax_9 -11.803*** (1.801) Observations 55,754 R2 0.942 Adjusted R2 0.883 Residual Std. Error 46.864 (df = 27659) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 That’s it. Do not bother to try to read into the regression results. Again, this is just an illustration of how R can be used to prepare a regression-ready dataset with spatial variables. Daymet website↩ daymetr vignette↩ See here for a fuller explanation of how to use the daymetr package.↩ This may not be ideal for a real research project because the original raw data is not kept. It is often the case that your econometric plan changes on the course of your project (e.g., using other weather variables or using different temporal aggregation of weather variables instead of monthly aggregation). When this happens, you need to download the same data all over again.↩ For parallelized computation, see Chapter ??↩ For Mac users, mclapply or pbmclapply (mclapply with progress bar) are good alternatives.↩ "],
["references.html", "References", " References "]
]
